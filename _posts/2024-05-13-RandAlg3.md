---
title: Randomized Algorithm â€” Concentration Inequalities
date: 2024-05-13 20:49:00 +500
math: true
categories: [Theory]
tags: [note, lang-en, algorithm]
---

## Introduction

In this article, we will introduce three important inequalities for possibility concentration. Then, we provide an example problem - Mean Estimation - to show the application of Chernoff bound.


## Inequalities


### Markov's Inequiality

Let $$X$$ be a non-negative *random variable* with mean $$\mu = \mathbb{E}X$$. Then we have:

$$ \forall a \geq 0: \Pr(X \geq a \mu) \leq \frac{1}{a} $$

$$ \text{Equivalently, } \forall a \geq 0: \Pr(X \geq a) \leq \frac{\mu}{a} $$



### Chebyshev's Inequality

Let $$X$$ be a non-negative *random variable* with mean $$\mu = \mathbb{E}X$$ and variance $$\delta^2 = \text{Var}(X)$$. Then we have:

$$ \forall a \geq 0: \Pr(|X-\mu| \geq a \delta) \leq \frac{1}{a^2} $$

$$ \text{Equivalently, } \forall a \geq 0: \Pr(|X-\mu| \geq a) \leq \frac{\delta^2}{a^2} $$


### Chernoff Bounds

Let $$X_1, ..., X_n$$ be independent random variables taking values in [0, 1]. Let $$ X = \sum_{i=1}^{n}X_i $$ be their sum with mean $$ \mu = \mathbb{E}X = \sum_{i=1}^{n}{E}X_i $$. Then we have:

$$ \forall \delta \geq 0: \Pr(X \geq (1+\delta) \mu) \leq (\frac{e^\delta}{(1+\delta)^{1+\delta}})^\mu $$

$$ \text{and } \Pr(X \leq (1-\delta) \mu) \leq (\frac{e^{-\delta}}{(1-\delta)^{1-\delta}})^\mu $$

A looser but simpler version (also is more useful in practice) is:

$$ \forall \delta \in (0,1): \Pr(X \geq (1+\delta) \mu) \leq e^{-\frac{\delta^2 \mu}{3}} $$

$$ \text{and } \Pr(X \leq (1-\delta) \mu) \leq e^{-\frac{\delta^2 \mu}{2}} $$


## Mean Estimation

### Problem Description

Suppose $$X_1, X_2, ...$$ are i.i.d. (independent and identically distributed)  of a random variable $$X$$. Our goal is to estimate $$\mathbb{E}X$$ from these samples. Moreover, we want to understand how many samples are needed so that our estimate $$\hat{X}$$ satisfies

$$ \Pr(|\hat{X} - \mathbb{E}X| \leq \varepsilon \mathbb{E}X) \geq 1- \delta \text{,}$$

where $$ \varepsilon, \delta \in (0,1) $$ are given. Here $$ \varepsilon $$ is the approximation error and $$\delta$$ is the failure probability.

### Simple Approach: Sample Mean

Suppose we have $$n$$ i.i.d. samples $$X_1, ..., X_n $$. Let $$ Y = \sum_{i=1}^{n}X_i $$ be their sum, so $$ \mathbb{E}Y = n\mathbb{E}X $$. Let $$\hat{X} = Y / n$$ be the sample mean, so $$ \mathbb{E}X = \mathbb{E}\hat{X} $$. We have

$$ \Pr(|\hat{X} - \mathbb{E}X| \geq \varepsilon \mathbb{E}X) =  \Pr(|Y - \mathbb{E}X| \geq \varepsilon \mathbb{E}X)$$

We *cannot* use the Chernoff bound here since $$X$$ may not be bounded (e.g., $$X$$ is geometric or Gaussian). Instead, we apply the Chebyshev's inequality:

$$ \Pr(|Y - \mathbb{E}X| \geq \varepsilon \mathbb{E}X) \leq \frac{\text{Var}(Y)}{(\varepsilon \mathbb{E}Y)^2} = \frac{n\text{Var}(X)}{(\varepsilon n\mathbb{E}X)^2} = \frac{r^2}{\varepsilon^2 n} \text{,}$$

where

$$ r = \frac{\sqrt{\text{Var}(X)}}{\mathbb{E}X}  \text{.}$$

We want the failure probability to be at most $$\delta$$, so it suffices to take

$$ \frac{r^2}{\varepsilon^2 n} \leq \delta \Longleftrightarrow n \geq \frac{r^2}{\varepsilon^2 \delta} \text{.} $$

Based on this result, we can know that the number of samples we need is $$ n = O(r^2 / (\varepsilon^2 \delta)) $$. The linear dependency on $$1/\delta$$ is bad for many applications. In the next section, we will give a better approach which achieves $$\log(1/\delta)$$ dependency for sample complexity.


### Better Approach: Median of Means

Assume we choose $$m = 4r^2/\varepsilon^2$$ samples and calculate the esimate $$ \hat{X} = \frac{1}{m} \sum_{i=1}^{m}X_i $$, we have:

$$ \Pr(|\hat{X} - \mathbb{E}X| \geq \varepsilon \mathbb{E}X) \leq \frac{r^2}{\varepsilon^2m} = \frac{1}{4} $$

We say $$ \hat{X} $$ is a **good** estimation if $$ \lvert \hat{X} - \mathbb{E}X \rvert \leq \varepsilon \mathbb{E}X$$. Therefore, $$ \Pr(\hat{X} \text{ is good}) \geq 3/4$$.

Our plan is to generate many such estimates $$\hat{X}$$ independently. Since each estimate is good with probability at least 3/4, the majority of these estimates would be good with high probability, and hence we can use the median of these estimates as our final estimator. We can formalize our idea into the following "Median of Means" algorithm.

![Median of Means]({{site.url}}/assets/img/2024-05-12/alg3.png){: width="700"}

The idea of this algorithm is to take means first and then take median of means as the final estimation. Using this algorithm, the number of samples we need is 

$$ n = lm =24\log{1/\delta} \cdot \frac{4r^2}{\varepsilon^2} = O(\frac{r2}{\varepsilon^2}log(1/\delta)) \text{.}$$

So, this approach indeed achieves $$\log(1/\delta)$$ dependency for sample complexity.


