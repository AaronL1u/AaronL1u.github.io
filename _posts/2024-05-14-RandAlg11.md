---
title: Randomized Algorithm XI â€” 2-SAT & Markov Chain
date: 2024-05-30 12:02:00 +500
math: true
categories: [Theory, Randomized Algorithm]
tags: [note, lang-en, algorithm]
---

A Boolean formula $$f$$ is in **conjunctive normal form (CNF)** if it is an AND of ORs, e.g.,

$$ f = (x_1 \lor \lnot x_3 \lor x_4) \land (\lnot x_2 \lor x_3) \land (x_2)$$

Given a Boolean formula $f$ in CNF where every clause contains exactly two literals, we want to find a satisfying assignment for $f$, and this problem is called the **2-SAT** problem.

SAT problem for CNF is NP-complete, but 2-SAT can be solved in polynomial time via a reduction to strongly connected components of directed graphs. 

Here we present a simple randomized algorithm. Algorithm 1 applies to the general SAT problem and we shall analyze it for 2-SAT.

![Monte Carlo Method]({{site.url}}/assets/img/2024-05-12/alg17.png){: width="610"}

There are some remarks about this algorithm:

1. We can pick an arbitrary assignment $\sigma$ as initialization, e.g., taking the all-true or all-false assignment. Alternatively, we can apply random initialization: start with a uniformly random assignment $\sigma$.

2. In Line 2, we can choose the unsatisfied clause $c$ arbitrarily, e.g., picking the one of the smallest index,
or choosing one uniformly at random.

3. In Line 3, we have to choose a literal uniformly at random for our analysis to work.

If f is unsatisfiable, then clearly Algorithm 1 will output Unsatisfiable. Suppose $f$ is satisfiable, and let $\tau$ be a satisfying assignment. Define $\sigma_t$ to be the assignment at time $t$ of Algorithm 1, and $X_t$ to be the
number of variables that agree between $\sigma_t$ and $\tau$.

> "Agree" means the variable is set to the same truth value in two assignments. For example, suppose $\sigma_t = (\text{T, T, F, T})$ and $\tau = (\text{T, F, F, T})$, we have $X_t = 3$.
{: .prompt-tip}

If $X_t = n$, then $\sigma_t = \tau$ and the algorithm finds a satisfying assignment. 

Note that $$X_t \in \{ 0,1,...,n \}$$ can be thought of as a random walk moving between adjacent integers where either $X_{t+1} = X_{t}+1$ or $X_{t+1} = X_{t}-1$ (because each walk will only change the value of **one** variable!).

Here we have a claim:

*For each* $t < T$ *and* $$i \in \{ 0,1,...,n-1 \}$$ *, we have*

$$\Pr(X_{t+1} = i+1 \mid X_t=i) \geq \frac{1}{2}.$$

>(Optional to read) **Proof.** Suppose we pick clause $c$ in the update from $\sigma_t$ to $\sigma_{t+1}$, and the two variables in $c$ are $x_1$ and $x_2$ without loss of generality. Then, $c$ is not satisfied by $\sigma_t$ but is satisfied by $\tau$. This means that $\sigma_t(x_j) \neq \tau(x_j)$ for at least one $$j \in \{ 1,2 \}$$. If $\delta_t(x_1) \neq \tau(x_1)$ and $\delta_t(x_2) = \tau(x_2)$, then $X_{t+1} = X_t + 1$ with probability 1/2. Similarly for the case $\delta_t(x_1) = \tau(x_1)$ and $\delta_t(x_2) \neq \tau(x_2)$. If $\delta_t(x_1) \neq \tau(x_1)$ and $\delta_t(x_2) \neq \tau(x_2)$, then $X_{t+1} = X_t + 1$ always. The claim then follows.
{: .prompt-info }

This claim indicates that for each walk we have larger chance to move forward (to the correct answer) than to move backward. Based on this claim, we want to further show that the number of steps for $X_t$ to reach $n$ (in which case $\sigma_t = \tau$) is $\text{poly}(n)$ in expectation.

To simplify the proof, let's consider a slowed-down process $(Y_t)$ where $Y_0 = X_0$ and 

$$\Pr(Y_{t+1} = i+1 \mid Y_t=i) = \frac{1}{2}.$$

So, $(Y_t)$ is an unbiased random walk on $$\{ 0,1,...,n \}$$. If we can show this slowed-down version can reach $n$ in expected $\text{poly}(n)$ steps, then $(X_t)$ can do as well! And this is what we are going to do.

Define $H_j$ to be the number of steps for the random walk $(Y_t)$ to reach $n$ when starting $Y_0 = j$. Let $h_j = \mathbb{E}[H_j]$ be its expectation. Here comes a key lemma.

**(Lemma)** For all $j \in \{ 0,1,...,n \}$, we have

$$h_j = n^2 - j^2.$$

In particular, $h_j \leq h_0 = n^2$.

**Proof.** By definition, we have the following recurrence:

$$
\begin{cases}
h_0 = h_1 + 1;\\
h_j = \frac{1}{2} h_{j-1} + \frac{1}{2} h_{j+1} + 1, \quad 1 \leq j \leq n-1;\\
h_n = 0.
\end{cases}
$$

Thus, we have 

$$ h_j-h_{j+1} = h_{j-1} - h_{j} + 2 = h_{j-2} - h_{j-1} + 4 = \cdots = h_0-h_1+2j = 2j + 1,$$

and 

$$ h_j = h_j-h_n = \sum_{i=j}^{n-1}h_i - h_{i+1} =  \sum_{i=j}^{n-1}2i + 1 = n^2-j^2,$$

as claimed.

If we run Algorithm 1 with $T = 2n^2$ rounds, then it holds

$$
\begin{split}
&\Pr(\text{Algorithm 1 outputs Unsatisfiable})\\
\leq &\Pr(X_t \text{ does not reach } n \text{ for } t \leq 2n^2)\\
\leq &\Pr(Y_t \text{ does not reach } n \text{ for } t \leq 2n^2)\\
\leq &\Pr(H_0 > 2n^2)\\
\leq & \frac{h_0}{2n^2} = \frac{1}{2}. \quad \quad \quad \quad \quad \text{(Markov's Inequality)}
\end{split}
$$

> The walk we described in this section is so called **Markov chain**. A Markov chain, simply speaking, is a stochastic process where the outcome of each step is based solely on the present state. More to see this [wiki page](https://en.wikipedia.org/wiki/Markov_chain).
{: .prompt-tip}





