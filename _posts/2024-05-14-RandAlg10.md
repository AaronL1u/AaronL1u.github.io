---
title: Randomized Algorithm X â€” Satisfying DNF
date: 2024-05-25 23:25:00 +500
math: true
categories: [Theory, Randomized Algorithm]
tags: [note, lang-en, algorithm]
---

## #DNF

A Boolean formula $$f$$ is in **disjunctive normal form (DNF)** if it is an OR of ANDs, e.g.,

$$ f = (x_1 )$$

It is easy to find a satisfying assignment for a formula in DNF: Take any clause, and find an assignment that satisfies all literals in that clause. For example, $$(x_1, x_2, x_3, x_4) = (\text{T, T, F, T})$$ is a satisfying assignment to
the formula $$f$$ above where the first and third clauses are satisfied.

Given a Boolean formula $$f$$ in DNF with $$n$$ variables $$x_1, ..., x_n$$ and $$m$$ clauses $$c1, ... ,cm$$, our goal is to find

$$N(f) := \text{number of satisfying assignments of } f$$

Exact counting for $$N(f)$$ is NP-complete, which is conjectured to be unsolvable in $$\text{poly}(n,m)$$ time. We want to have an approximate counting algorithm for this problem.

Given a Boolean formula $$f$$ in DNF and an approximation parameter $$\varepsilon \in (0,1)$$ as inputs, a randomized algorithm is called a **fully polynomial-time randomized approximation scheme (FPRAS)** if it outputs $$\hat{N}$$ satisfying

$$\Pr((1-\varepsilon)\hat{N} \leq N(f) \leq (1+\varepsilon)\hat{N}) \geq \frac{3}{4}$$

in time $$\text{ploy}(n,m,1/\varepsilon)$$.

Here are some remarks about FPRAS:

1. An FPRAS gives the strongest form of approximation. It can achieve **WAITING TO BE FINISHED** approximation for any $$\varepsilon \in (0,1)$$, but the running time depends (inverse polynomially) on $$\varepsilon$$.

2. The running time of an FPRAS is fully polynomial in $$n$$, $$m$$, $$1/\varepsilon$$. Note, $$n^3m^{2/\varepsilon}$$, $$e^{1/\varepsilon}$$, $$(nm)^{\log{(1/\varepsilon)}} = (1/\varepsilon)^{\log{(nm)}}$$ are **not** fully polynomial.

3. We can boost the success probability of an FPRAS to $$1-\delta$$ for any (input) failure probability $$\delta \in (0,1)$$. This can be achieved by running $$O(\log{(1/\delta)})$$ trials of the FPRAS, and taking the median of all outputs (same analysis as the "median of means" approach by Chernoff bounds).

## Monte Carlo Method

Let $$\Omega$$ be a finite universe with "simple" structure such that we know $$\lvert \Omega \rvert$$ and can generate random element from $$\Omega$$. Let $$S \subseteq \Omega$$ be a subset for which we want to estimate $$\lvert \Omega \rvert$$. 


To estimate $$\lvert \Omega \rvert$$, the Monte Carlo algorithm generates a few independent samples from $$\Omega$$ uniformly at random and finds the fraction of samples that lie in $$S$$ which approximates $$\lvert S \rvert / \lvert \Omega \rvert$$. The detailed algorithm is shown below.

![Monte Carlo Method]({{site.url}}/assets/img/2024-05-12/alg15.png){: width="850"}

Let $$\alpha = \lvert S \rvert / \lvert \Omega \rvert$$, and observe that $$\mathbb{E}[Y_i] = \Pr(X_i \in S) = \lvert S \rvert / \lvert \Omega \rvert = \alpha$$ for each $$i$$. So, we have $$\mathbb{E}[\hat{Y}] = \alpha$$ and $$\mathbb{E}[\hat{N}] = \lvert \Omega \rvert \cdot \mathbb{E}[\hat{Y}] = \alpha \lvert \Omega \rvert = \lvert S \rvert$$. This shows that $$\hat{N}$$ is an unbiased estimator. By the Chernoff bounds, we deduce that

$$
\begin{split}
\Pr(|\hat{N}-|S||\geq \varepsilon |S|) = \Pr(|\frac{|\Omega|}{t}\sum_{i=1}^{t}Y_i - |S|| \geq \varepsilon |S|) \\
\geq \frac{1}{4}
\end{split}
$$


For the #DNF problem, we may define $$\Omega$$ to be the set of all truth assignments, so that $$\lvert \Omega \rvert = 2^n$$ and we can easily sample an assignment from $$\Omega$$ uniformly at random; and we define $$S \subseteq \Omega$$ to be the set of satisfying assignments, so $$N(f) = \lvert \Omega \rvert$$. 




## Better Monte Carlo Method for #DNF

For each clause $$c_i$$, let $$S_i$$ be the set of assignments that satisfy $$c_i$$. Let $$S = \bigcup_{i=1}^{m}S_i$$ be the set of all satisfying assignments. Note that $$N(f) = \lvert S \rvert$$.

We shall pick the universe $$\Omega$$ in a smarter way to apply Algorithm 1. Let $$\Omega$$ be the multiset union of $$S_i$$'s; specifically,

$$ \Omega = \{ (i, \delta) \in [m] \times S : \sigma \in S_i \}. $$

The set $$S$$ is technically not a subset of the universe $$\Omega$$, but we can easily find a bijective mapping from $$S$$ to a subset $$S' \subseteq \Omega$$ defined as

$$ S'= \{ (i, \delta) \in [m] \times S : \sigma \notin S_i \text{ for } j=1,...,i-1 \text{ and } \delta \in S_i \} $$

In other words, $$S'$$ consists of all pairs $$(i,\delta)$$ where $$c_i$$ is the first clause satisfied by $$\delta$$.

Here are some observations:

1. $$\lvert S' \rvert = \lvert S \rvert = N(f)$$;

2. $$S' \subseteq \Omega  \subseteq [m] \times S$$ and so $$ \lvert S' \rvert \leq \lvert \Omega \rvert \leq m \lvert S' \rvert$$;

3. For each $$i$$, we can easily sample u.a.r. from $$S_i$$ by satisfying all the literals in $$c_i$$ and choosing a uniformly random assignment for the renaming variables;

4. For each $$i$$, if $$\lvert c_i \rvert = k_i$$ (i.e., the clause $$c_i$$ contains $$k_i$$ literals), then $$\lvert S_i \rvert = 2^{n-k_i}$$.

From these observations, we can compute 

$$ |\Omega| = \sum_{i=1}^{m} |S_i| =  \sum_{i=1}^{m} 2^{n-k_i}, $$

and we can sample uniformly random elements from $$\Omega$$ by first sampling $$i \in [m]$$ with probability proportional to $$\lvert S_i \rvert$$, and then sampling $$\sigma$$ u.a.r. from $$S_i$$. Algorithm 1 is then specified to the following version.

![Better Monte Carlo Method]({{site.url}}/assets/img/2024-05-12/alg16.png){: width="850"}

Notice that 

$$ \mathbb{E}[\hat{Y}] = \alpha = \frac{|S'|}{|\Omega|} \geq \frac{1}{m} $$

by our second observation. Therefore, if we set $$ t= $$.